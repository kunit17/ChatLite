{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96485280b3b4e77811252d16492b210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main characters in the Disney movie \"Frozen\" are:\\n\\n1. **Elsa** (voiced by Idina Menzel): The elder sister of Anna, Elsa is the princess of Arendelle and has the magical power to create and control ice and snow. She is the main protagonist of the story.\\n2. **Anna** (voiced by Kristen Bell): The younger sister of Elsa, Anna is a optimistic, energetic, and determined princess who sets out to find Elsa and convince her to return home and reverse the eternal winter that has been cast upon Arendelle.\\n\\nHowever, if you\\'re asking about the main character of the movie, I would say that it\\'s a bit of a debate among fans, but many would argue that the main character is actually **Elsa**. Her journey and struggles with her powers and her relationship with Anna drive the plot of the movie.\\n\\nWhat do you think?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna is the blonde character in the Disney movie \"Frozen\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The snowman\\'s name is Olaf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olaf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olaf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m question\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m sequences, response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_bot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m chat_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(sequences)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m, in \u001b[0;36mChatGenerator.generate_response\u001b[1;34m(self, question)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[0;32m     47\u001b[0m     match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m|end_header_id\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m|>\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn(.*?)\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(sequence), re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m---> 48\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Gets everything after the last '<|end_header_id|>'\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequences, response\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import re\n",
    "import gradio as gr\n",
    "\n",
    "class ChatGenerator:\n",
    "    def __init__(self):\n",
    "        self.checkpoint = (Path.cwd()).parent / 'Llama-3.2-3B-Instruct' # Set the model checkpoint path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint) # Load tokenizer\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.checkpoint,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda\"  # Automatically uses GPU if available\n",
    "        )\n",
    "        self.text_generator = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda\"  # Auto-detect and use GPU\n",
    "        )\n",
    "\n",
    "    def generate_response(self, question: str):\n",
    "        prompt = (\n",
    "            \"<|begin_of_text|>\"\n",
    "            \"<|start_header_id|>user<|end_header_id|>\"\n",
    "            f\"{question}\"\n",
    "            \"<|eot_id|>\"\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        )\n",
    "\n",
    "        # Generate sequences\n",
    "        sequences = self.text_generator(\n",
    "            prompt,\n",
    "            do_sample=True,\n",
    "            top_k=2,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            truncation=True,\n",
    "            max_length=6000,\n",
    "        )\n",
    "\n",
    "        # Extract and return generated responses\n",
    "        for sequence in sequences:\n",
    "            match = re.search(r'<\\|end_header_id\\|>\\\\n\\\\n(.*?)\\'\\}', str(sequence), re.DOTALL)\n",
    "            response = match.group(1)  # Gets everything after the last '<|end_header_id|>'\n",
    "        return sequences, response\n",
    "\n",
    "# Usage example\n",
    "chat_bot = ChatGenerator()\n",
    "chat_history = \"\"\n",
    "\n",
    "\n",
    "while True:\n",
    "    question = input(\"Enter your question: \")\n",
    "    if question.lower() == \"exit\":\n",
    "        break\n",
    "    sequences, response = chat_bot.generate_response(chat_history + question)\n",
    "    chat_history += str(sequences)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
