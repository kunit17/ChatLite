{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9236e14543374a0dbc76f68978d97f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Directory containing PDFs\n",
    "pdf_dir = (Path.cwd()) / 'Data'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not pdf_dir.exists():\n",
    "    raise ValueError(f\"Directory {pdf_dir} does not exist\")\n",
    "\n",
    "# Load all PDFs from the directory\n",
    "chunked_docs = []\n",
    "\n",
    "for pdf_path in pdf_dir.glob(\"*.pdf\"):\n",
    "    loader = PyPDFLoader(str(pdf_path))\n",
    "    async for page in loader.alazy_load():\n",
    "        chunked_docs.append(page)\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "#from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "#db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\"))\n",
    "db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "class ChatGenerator:\n",
    "    def __init__(self):\n",
    "        self.checkpoint = (Path.cwd()).parent / 'Llama-3.2-3B-Instruct' # Set the model checkpoint path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.checkpoint) # Load tokenizer\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.checkpoint,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda\"  # Automatically uses GPU if available\n",
    "        )\n",
    "        print(self.model.config.pad_token_id)\n",
    "        self.text_generator = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda\"  # Auto-detect and use GPU\n",
    "        )\n",
    "\n",
    "    def generate_response(self, question: str):\n",
    "        prompt = (\n",
    "            \"<|begin_of_text|>\"\n",
    "            \"<|start_header_id|>user<|end_header_id|>\"\n",
    "            f\"{question}\\n\\nContext:\\n{context}\"\n",
    "            \"<|eot_id|>\"\n",
    "            \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        )\n",
    "\n",
    "        # Generate sequences\n",
    "        sequences = self.text_generator(\n",
    "            prompt,\n",
    "            do_sample=True,\n",
    "            top_k=2,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            truncation=True,\n",
    "            max_length=12000,\n",
    "        )\n",
    "\n",
    "        # Extract and return generated responses\n",
    "        for sequence in sequences:\n",
    "            match = re.search(r'<\\|end_header_id\\|>\\\\n\\\\n(.*?)\\'\\}', str(sequence), re.DOTALL)\n",
    "            response = match.group(1)  # Gets everything after the last '<|end_header_id|>'\n",
    "        return sequences, response\n",
    "\n",
    "# Usage example\n",
    "chat_bot = ChatGenerator()\n",
    "chat_history = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barbu\\AppData\\Local\\Temp\\ipykernel_22016\\3116917095.py:5: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_docs = retriever.get_relevant_documents(question)\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(question)\n\u001b[0;32m      6\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m relevant_docs])\n\u001b[1;32m----> 7\u001b[0m sequences, response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_bot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m chat_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(sequences)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[2], line 78\u001b[0m, in \u001b[0;36mChatGenerator.generate_response\u001b[1;34m(self, question)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[0;32m     77\u001b[0m     match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m|end_header_id\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m|>\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn(.*?)\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(sequence), re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m---> 78\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Gets everything after the last '<|end_header_id|>'\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sequences, response\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"Enter your question: \")\n",
    "    if question.lower() == \"exit\":\n",
    "        break\n",
    "    relevant_docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    sequences, response = chat_bot.generate_response(chat_history + question + context)\n",
    "    chat_history += str(sequences)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
